{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fa5c956-8a44-4632-b0b2-ad4e5e6a4650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = \"/Users/aditrichaudhuri/Downloads/input_base_html_13271490.html\"\n",
    "file_path=\"/Users/aditrichaudhuri/Downloads/input_base_html4.html\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    html_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81b67410-13f2-4a95-83b4-ddf0ad317c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order Number: 13271000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "order_number = re.search(r'<td>(\\d{8})</td>', html_text)\n",
    "if order_number:\n",
    "    print(\"Order Number:\", order_number.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "183f2295-5805-43f6-84d1-879aa50268e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129671\n",
      "B2\n",
      "CUSTOM EARTH PROMOS\n",
      "601 N CONGRESS AVE\n",
      "STE 605\n",
      "DELRAY BEACH FL 33445 4621\n"
     ]
    }
   ],
   "source": [
    "def extract_bill_to_lines(html_text):\n",
    "    match = re.search(\n",
    "        r'<u>Bill To:</u>.*?>(\\d+)</a>\\s*(.*?)<br>(.*?)<br>(.*?)<br>(.*?)<br>(.*?)<',\n",
    "        html_text,\n",
    "        re.DOTALL\n",
    "    )\n",
    "    if match:\n",
    "        lines = [\n",
    "            match.group(1).strip(),  \n",
    "            match.group(2).strip(),  \n",
    "            match.group(3).strip(),  \n",
    "            match.group(4).strip(), \n",
    "            match.group(5).strip(),  \n",
    "            match.group(6).strip()   \n",
    "        ]\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "            \n",
    "extract_bill_to_lines(html_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "853eec1b-54ce-40f6-9388-861458bc8124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer PO Number: 36994\n"
     ]
    }
   ],
   "source": [
    "po_number = re.search(r'<td>(\\d+)</td>\\s*<td class=\"gap\">', html_text)\n",
    "if po_number:\n",
    "    print(\"Customer PO Number:\", po_number.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c9300af8-d8e9-4fbd-b7bf-bc5103ab9750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship_to:\n",
      "AMY MAYNE\n"
     ]
    }
   ],
   "source": [
    "ship_to_match = re.search(\n",
    "    r'<u>Ship To:</u>\\s*(\\d+)<br>\\s*(.*?)<br>(.*?)<br>(.*?)<br>(.*?)<',\n",
    "    html_text, re.DOTALL)\n",
    "if ship_to_match:\n",
    "    print(\"ship_to:\")\n",
    "    print(ship_to_match.group(2).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8fa6f189-13cd-4f4c-9c27-09c5bb827f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Ship Date: 2/18/25\n"
     ]
    }
   ],
   "source": [
    "est_ship = re.search(r'id=\"traveler-required-date-.*?\">([\\d/]+)</td>', html_text)\n",
    "if est_ship:\n",
    "    print(\"Estimated Ship Date:\", est_ship.group(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2a2ce1fe-fe63-470d-b6c8-fd8789c13e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carrier: FEDEX GROUND\n"
     ]
    }
   ],
   "source": [
    "carrier = re.search(r'id=\"traveler-carrier-code-.*?\">([\\w\\s]+)</td>', html_text)\n",
    "if carrier:\n",
    "    print(\"Carrier:\", carrier.group(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "30feadba-f6ec-4de6-8efa-8a4ec23f1292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email_proof:\n",
      "HALLE@CUSTOMEARTHPROMOS.COM\n"
     ]
    }
   ],
   "source": [
    "email_proof_match = re.search(\n",
    "    r'EMAIL PROOF TO:\\s*([\\w\\.-]+@[\\w\\.-]+)', html_text)\n",
    "if email_proof_match:\n",
    "    print(\"email_proof:\")\n",
    "    print(email_proof_match.group(1).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "32dee8f1-8869-4b5e-bd2c-5ddd0ace875c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ord_qty:\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "ord_qty_match = re.search(\n",
    "    r'<td>1\\.00</td>\\s*<td>(\\d+)</td>',\n",
    "    html_text)\n",
    "if ord_qty_match:\n",
    "    print(\"ord_qty:\")\n",
    "    print(ord_qty_match.group(1).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c750a2a0-b1d1-4d17-b0ae-1dd276bf9a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_hands:\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "in_hands_match = re.search(\n",
    "    r'id=\"traveler-in-hands-date-[^\"]+\">.*?<div[^>]*>(.*?)</div>',\n",
    "    html_text,\n",
    "    re.DOTALL\n",
    ")\n",
    "if in_hands_match:\n",
    "    print(\"in_hands:\")\n",
    "    print(in_hands_match.group(1).strip())\n",
    "else:\n",
    "    print(\"in_hands not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "32b5a63a-919c-4d31-b630-c4926518475e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instructions_method:\n",
      "386228973\n"
     ]
    }
   ],
   "source": [
    "instructions_method_match = re.search(\n",
    "    r'id=\"traveler-ship-instructions-[^\"]+\" colspan=\"2\">([\\d]+)</td>',\n",
    "    html_text)\n",
    "if instructions_method_match:\n",
    "    print(\"instructions_method:\")\n",
    "    print(instructions_method_match.group(1).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97c127-9c5d-4a26-ab10-cc9a385b6c06",
   "metadata": {},
   "source": [
    "**using Keywords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "17f81e62-2cf5-43fa-b11c-d7c8c17411ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship_to:\n",
      "AMY MAYNE\n"
     ]
    }
   ],
   "source": [
    "ship_to_match = re.search(\n",
    "    r'<u>Ship To:</u>\\s*(\\d+)<br>\\s*(.*?)<br>\\s*(.*?)<br>\\s*(.*?)<br>\\s*(.*?)<',\n",
    "    html_text, re.DOTALL)\n",
    "if ship_to_match:\n",
    "    print(\"ship_to:\")\n",
    "    print(ship_to_match.group(2).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18e74cc9-3533-40c4-a301-e1d922700281",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\n",
    "    r'PRODUCT\\s*:\\s*(.*?)\\s*<br>\\s*DESCRIPTION\\s*:\\s*(.*?)\\s*(?:<br>|</p>)',\n",
    "    re.DOTALL | re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Extract matches\n",
    "matches = pattern.findall(html_text)\n",
    "\n",
    "# Display extracted results\n",
    "for i, (product, description) in enumerate(matches, start=1):\n",
    "    print(f\"{i}. PRODUCT: {product.strip()}\")\n",
    "    print(f\"   DESCRIPTION: {description.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "151ba44e-0207-4702-984a-06a6a4d8e4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. PRODUCT: HOT STAMPING(5800)\n",
      "   DESCRIPTION: *** E-Traveler ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pattern = re.compile(\n",
    "    r'<div class=\"section a-center\">\\s*(.*?)<br>\\s*Qty:\\s*\\d+<br>\\s*(.*?)\\s*<',\n",
    "    re.DOTALL | re.IGNORECASE\n",
    ")\n",
    "\n",
    "matches = pattern.findall(html_text)\n",
    "\n",
    "# Output results\n",
    "if matches:\n",
    "    for i, (product, description) in enumerate(matches, start=1):\n",
    "        print(f\"{i}. PRODUCT: {product.strip()}\")\n",
    "        print(f\"   DESCRIPTION: {description.strip()}\\n\")\n",
    "else:\n",
    "    print(\"No matches found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c14af1e-0afd-4e73-97af-f799a783a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "with open('/Users/aditrichaudhuri/Downloads/input_base_html2.html', 'r', encoding='utf-8') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "\n",
    "rows = soup.find_all('tr')\n",
    "\n",
    "# Defining keywords to anchor the middle section \n",
    "start_field = \"State\"\n",
    "end_field = \"Post Office Name\"\n",
    "extracting = False\n",
    "\n",
    "middle_data = {}\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all(['td', 'th'])\n",
    "    text = [col.get_text(strip=True) for col in cols]\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    if any(start_field in t for t in text):\n",
    "        extracting = True\n",
    "\n",
    "    if extracting and len(text) >= 2:\n",
    "        field_name = text[0]\n",
    "        field_value = text[1]\n",
    "        middle_data[field_name] = field_value\n",
    "\n",
    "    if any(end_field in t for t in text):\n",
    "        break\n",
    "\n",
    "\n",
    "for key, value in middle_data.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10890d9b-5353-47d5-81ff-c3a82125c94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bill To: 129671 B2 CUSTOM EARTH PROMOS601 N CONGRESS AVE STE 605DELRAY BEACH FL 33445 4621', 'WHSE: 10 LARGO, FL', 'Ship To: 129671 AMY MAYNE3828 FRUIT AVEMEDINA NY 14103 9568 Google Maps']\n",
      "['Ord#/Date', 'Cust Po#', 'Est Ship', 'In Hands', 'Instructions/Method']\n",
      "['13250710', '36994', '2/18/25', '-', '386228973']\n",
      "['2/11/25', 'B/O: 00', 'FEDEX GROUND']\n",
      "['Ship Time', 'Saturday?', 'Route']\n",
      "['19:00:00', 'NO', '0']\n",
      "['Line#', 'Ord Qty', 'Ship Qty', 'Product#', 'Description', 'Net Unit$']\n",
      "\n",
      "--- Extracted Fields ---\n",
      "Line#: Ord Qty\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "with open('/Users/aditrichaudhuri/Downloads/input_base_html2.html', 'r', encoding='utf-8') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "rows = soup.find_all('tr')\n",
    "start_field = \"Ship Qty\"\n",
    "end_field = \"Net Unit$\"\n",
    "\n",
    "extracting = False\n",
    "middle_data = {}\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all(['td', 'th'])\n",
    "    text_cols = [clean_text(col.get_text()) for col in cols if clean_text(col.get_text())]\n",
    "\n",
    "    if not text_cols:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    print(text_cols)\n",
    "\n",
    "    if any(start_field in col for col in text_cols):\n",
    "        extracting = True\n",
    "\n",
    "    if extracting and len(text_cols) >= 2:\n",
    "        field_name = text_cols[0]\n",
    "        field_value = text_cols[1]\n",
    "        middle_data[field_name] = field_value\n",
    "\n",
    "    if any(end_field in col for col in text_cols):\n",
    "        break\n",
    "\n",
    "print(\"\\n--- Extracted Fields ---\")\n",
    "for key, value in middle_data.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d999857d-9e3a-4833-aa93-e4fe50f01f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Extracted Product Info ---\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "with open('/Users/aditrichaudhuri/Downloads/input_base_html2.html', 'r', encoding='utf-8') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "rows = soup.find_all('tr')\n",
    "\n",
    "# Target fields\n",
    "target_fields = [\"Product#\", \"Description\"]\n",
    "extracted_data = {}\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all(['td', 'th'])\n",
    "    text_cols = [clean_text(col.get_text()) for col in cols if clean_text(col.get_text())]\n",
    "\n",
    "    if len(text_cols) >= 2:\n",
    "        label, value = text_cols[0], text_cols[1]\n",
    "        if any(field.lower() in label.lower() for field in target_fields):\n",
    "            extracted_data[label] = value\n",
    "\n",
    "print(\"--- Extracted Product Info ---\")\n",
    "for k, v in extracted_data.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fc436f71-dd11-4ca7-8551-39efd834a718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Extracted Fields via Bigrams ---\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from itertools import tee\n",
    "\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "def get_bigrams(words):\n",
    "    a, b = tee(words)\n",
    "    next(b, None)\n",
    "    return [' '.join(pair) for pair in zip(a, b)]\n",
    "\n",
    "# Load HTML\n",
    "with open('/Users/aditrichaudhuri/Downloads/input_base_html2.html', 'r', encoding='utf-8') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "rows = soup.find_all('tr')\n",
    "\n",
    "# Define expected fields (can be unigrams or bigrams)\n",
    "target_keywords = ['State', 'District', 'Post Office', 'Product Description', 'Sub Division']\n",
    "\n",
    "extracted_data = {}\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all(['td', 'th'])\n",
    "    text_cols = [clean_text(col.get_text()) for col in cols if clean_text(col.get_text())]\n",
    "    \n",
    "    if len(text_cols) >= 2:\n",
    "        label = text_cols[0]\n",
    "        value = text_cols[1]\n",
    "\n",
    "        # Generate unigrams and bigrams from label\n",
    "        words = label.split()\n",
    "        bigrams = get_bigrams(words)\n",
    "        candidates = set(words + bigrams)\n",
    "\n",
    "        # Check if any of the candidates match our target\n",
    "        if any(target.lower() in candidate.lower() for target in target_keywords for candidate in candidates):\n",
    "            extracted_data[label] = value\n",
    "\n",
    "# Print the results\n",
    "print(\"\\n--- Extracted Fields via Bigrams ---\")\n",
    "for key, value in extracted_data.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e1ae6-3f19-4ecb-92ff-e427bed80f97",
   "metadata": {},
   "source": [
    "**APPROACH 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c3040249-1252-494e-a5cd-0e91b2546cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Extracted Product Lines ---\n",
      "Line 1: ['LIM LIME 170', 'SS SILK SCREEN', 'BLACK COLOR 1 - LOC1']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from itertools import tee\n",
    "\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "with open('/Users/aditrichaudhuri/Downloads/input_base_html2.html', 'r', encoding='utf-8') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "rows = soup.find_all('tr')\n",
    "\n",
    "all_lines = []\n",
    "current_line = []\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all(['td', 'th'])\n",
    "    text_cols = [clean_text(col.get_text()) for col in cols if clean_text(col.get_text())]\n",
    "\n",
    "    # If it's numeric or alphanumeric and short, it's likely part of a product line\n",
    "    if text_cols:\n",
    "        if len(text_cols) == 1 and (text_cols[0].replace('.', '').isdigit() or len(text_cols[0]) <= 20):\n",
    "            current_line.append(text_cols[0])\n",
    "        elif len(text_cols) == 2:\n",
    "            current_line.extend(text_cols)\n",
    "        \n",
    "        # Heuristic: if line has 3–4 items, assume complete product entry\n",
    "        if len(current_line) >= 3:\n",
    "            all_lines.append(current_line)\n",
    "            current_line = []\n",
    "\n",
    "# Print results\n",
    "print(\"\\n--- Extracted Product Lines ---\")\n",
    "for i, line in enumerate(all_lines, 1):\n",
    "    print(f\"Line {i}: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b4cfd-c98e-4997-a1ac-4ac07eaa708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRACT imprint method AND color\n",
    "# using text and using image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9889fac-e7df-4d03-8513-b0414ed4186c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom bs4 import BeautifulSoup\\n\\ndef clean_text(text):\\n    # If text is None or empty, return an empty string;\\n    # otherwise, strip extra spaces and normalize whitespace.\\n    return \\' \\'.join(text.strip().split()) if text else \\'\\'\\n\\nwith open(\\'/Users/aditrichaudhuri/Downloads/input_base_html2.html\\', \\'r\\', encoding=\\'utf-8\\') as f:\\n    soup = BeautifulSoup(f, \\'html.parser\\')\\n\\n# Find all table row elements (each <tr>)\\nrows = soup.find_all(\\'tr\\')\\n\\nall_lines = []       # To hold the complete set of product lines\\ncurrent_line = []    # To temporarily collect cells for one product\\n\\nfor row in rows:\\n    # Extract text from each table cell in the row (<td> and <th>)\\n    # Ensure we only pick non-empty, cleaned text strings.\\n    text_cols = [clean_text(col.get_text()) for col in row.find_all([\\'td\\', \\'th\\']) if clean_text(col.get_text())]\\n\\n    # Skip rows with no valid text\\n    if not text_cols:\\n        continue\\n\\n    # --- Robust logic to determine if the row is a small piece or a combined row ---\\n    # Case 1: If there is only one text cell and:\\n    #         either it\\'s numeric (ignoring possible dots) or it is a short string (<=20 characters)\\n    if len(text_cols) == 1 and (text_cols[0].replace(\\'.\\', \\'\\').isdigit() or len(text_cols[0]) <= 20):\\n        current_line.append(text_cols[0])\\n    # Case 2: If the row has exactly two pieces of text, add both.\\n    elif len(text_cols) == 2:\\n        current_line.extend(text_cols)\\n    # Case 3: If the row has more than two pieces, you might be seeing an entire product entry all at once.\\n    #         In that case, it might be best to add the whole list or a subset, based on your heuristic.\\n    else:\\n        # Here, we choose to add all of them.\\n        current_line.extend(text_cols)\\n\\n    # --- Heuristic check ---\\n    # Assume that a complete product entry typically consists of 3 or more parts.\\n    # Once the current_line meets or exceeds 3 items, treat it as a complete product entry.\\n    if len(current_line) >= 3:\\n        all_lines.append(current_line)\\n        current_line = []  # Reset for the next entry\\n\\n# Optional: If there\\'s leftover content in \\'current_line\\' that didn\\'t reach 3 items,\\n# you may want to decide whether to include it or discard it.\\nif current_line:\\n    all_lines.append(current_line)\\n\\n# For debugging: print the extracted product lines.\\nprint(\"\\n--- Extracted Product Lines ---\")\\nfor i, line in enumerate(all_lines, 1):\\n    print(f\"Line {i}: {line}\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    # If text is None or empty, return an empty string;\n",
    "    # otherwise, strip extra spaces and normalize whitespace.\n",
    "    return ' '.join(text.strip().split()) if text else ''\n",
    "\n",
    "with open('/Users/aditrichaudhuri/Downloads/input_base_html2.html', 'r', encoding='utf-8') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "# Find all table row elements (each <tr>)\n",
    "rows = soup.find_all('tr')\n",
    "\n",
    "all_lines = []       # To hold the complete set of product lines\n",
    "current_line = []    # To temporarily collect cells for one product\n",
    "\n",
    "for row in rows:\n",
    "    # Extract text from each table cell in the row (<td> and <th>)\n",
    "    # Ensure we only pick non-empty, cleaned text strings.\n",
    "    text_cols = [clean_text(col.get_text()) for col in row.find_all(['td', 'th']) if clean_text(col.get_text())]\n",
    "\n",
    "    # Skip rows with no valid text\n",
    "    if not text_cols:\n",
    "        continue\n",
    "\n",
    "    # --- Robust logic to determine if the row is a small piece or a combined row ---\n",
    "    # Case 1: If there is only one text cell and:\n",
    "    #         either it's numeric (ignoring possible dots) or it is a short string (<=20 characters)\n",
    "    if len(text_cols) == 1 and (text_cols[0].replace('.', '').isdigit() or len(text_cols[0]) <= 20):\n",
    "        current_line.append(text_cols[0])\n",
    "    # Case 2: If the row has exactly two pieces of text, add both.\n",
    "    elif len(text_cols) == 2:\n",
    "        current_line.extend(text_cols)\n",
    "    # Case 3: If the row has more than two pieces, you might be seeing an entire product entry all at once.\n",
    "    #         In that case, it might be best to add the whole list or a subset, based on your heuristic.\n",
    "    else:\n",
    "        # Here, we choose to add all of them.\n",
    "        current_line.extend(text_cols)\n",
    "\n",
    "    # --- Heuristic check ---\n",
    "    # Assume that a complete product entry typically consists of 3 or more parts.\n",
    "    # Once the current_line meets or exceeds 3 items, treat it as a complete product entry.\n",
    "    if len(current_line) >= 3:\n",
    "        all_lines.append(current_line)\n",
    "        current_line = []  # Reset for the next entry\n",
    "\n",
    "# Optional: If there's leftover content in 'current_line' that didn't reach 3 items,\n",
    "# you may want to decide whether to include it or discard it.\n",
    "if current_line:\n",
    "    all_lines.append(current_line)\n",
    "\n",
    "# For debugging: print the extracted product lines.\n",
    "print(\"\\n--- Extracted Product Lines ---\")\n",
    "for i, line in enumerate(all_lines, 1):\n",
    "    print(f\"Line {i}: {line}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8ab46-cce6-4a77-9e42-7eeb0627c70a",
   "metadata": {},
   "source": [
    "**APPROACH 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b662a059-7bcd-42f5-aa0a-5aa9953baac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color: Kan-Tastic\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "with open('/Users/aditrichaudhuri/Downloads/input_base_html6.html', 'r', encoding='utf-8') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "rows = soup.find_all('tr')\n",
    "\n",
    "# Gathering lines\n",
    "all_lines = []\n",
    "current_line = []\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all(['td', 'th'])\n",
    "    text_cols = [clean_text(col.get_text()) for col in cols if clean_text(col.get_text())]\n",
    "\n",
    "    if text_cols:\n",
    "        if len(text_cols) == 1 and (text_cols[0].replace('.', '').isdigit() or len(text_cols[0]) <= 20):\n",
    "            current_line.append(text_cols[0])\n",
    "        elif len(text_cols) == 2:\n",
    "            current_line.extend(text_cols)\n",
    "\n",
    "        if len(current_line) >= 3:\n",
    "            all_lines.append(current_line)\n",
    "            current_line = []\n",
    "\n",
    "color = None\n",
    "\n",
    "if all_lines:\n",
    "    first_field = all_lines[0][0]  \n",
    "    words = first_field.split()\n",
    "    if len(words) >= 2:\n",
    "        color = words[1]  # Picking the second word\n",
    "\n",
    "print(\"Color:\", color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c99f2a1-fb78-4d28-9d59-829d568ce929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['#34 Kan-Tastic', 'FRONT FRONT', 'SS SILK SCREEN'], ['FRONT FRONT', 'PP PAD PRINT', 'BLK BLACK 021'], ['SS SILK SCREEN', 'BARREL BARREL', 'HP HP PRINTER'], ['RED RED 258', 'SS SILK SCREEN', 'Notes:', '(SH)NO OVERS(SH)NO UNDERS ALLOWED']]\n"
     ]
    }
   ],
   "source": [
    "print(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc7a1e64-29f7-455c-90c1-a4b97e821e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imprint Method:\n",
      "SIDE1\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "with open('/Users/aditrichaudhuri/Downloads/input_base_html5.html', 'r', encoding='utf-8') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "rows = soup.find_all('tr')\n",
    "\n",
    "\n",
    "all_lines = []\n",
    "current_line = []\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all(['td', 'th'])\n",
    "    text_cols = [clean_text(col.get_text()) for col in cols if clean_text(col.get_text())]\n",
    "\n",
    "    if text_cols:\n",
    "        if len(text_cols) == 1 and (text_cols[0].replace('.', '').isdigit() or len(text_cols[0]) <= 20):\n",
    "            current_line.append(text_cols[0])\n",
    "        elif len(text_cols) == 2:\n",
    "            current_line.extend(text_cols)\n",
    "\n",
    "        if len(current_line) >= 3:\n",
    "            all_lines.append(current_line)\n",
    "            current_line = []\n",
    "\n",
    "\n",
    "imprint_method = None\n",
    "\n",
    "\"\"\"\n",
    "if all_lines:\n",
    "    second_field = all_lines[0][1]  \n",
    "    words = second_field.split()\n",
    "    if len(words) >= 2:\n",
    "        imprint_method = ' '.join(words[1:])  \n",
    "\n",
    "print(\"Imprint Method:\")\n",
    "print(imprint_method)\n",
    "\"\"\"\n",
    "\n",
    "if all_lines:\n",
    "    second_field = all_lines[0][1]\n",
    "    if second_field =='SIDE1':\n",
    "        third_field = all_lines[0][2]\n",
    "        words = third_field.split()\n",
    "        if len(words) >= 2:\n",
    "            imprint_method = ' '.join(words[1:])  \n",
    "    else:\n",
    "        words = second_field.split()\n",
    "        if len(words) >= 2:\n",
    "            imprint_method = ' '.join(words[1:])  \n",
    "\n",
    "print(\"Imprint Method:\")\n",
    "print(imprint_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b0567-866c-440a-a57b-8c6280b2d78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b81bd387-ee3d-41da-ae6b-23752d772b16",
   "metadata": {},
   "source": [
    "**APPROACH 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49c21fcc-edfd-4da9-b59d-b7dc299f6c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Color: SILVER\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# a set of basic color names (extendable)\n",
    "color_names = {\n",
    "    'BLACK', 'WHITE', 'RED', 'BLUE', 'GREEN', 'YELLOW', 'ORANGE', 'PURPLE', 'PINK', 'BROWN', 'GRAY',\n",
    "    'LIME', 'CYAN', 'NAVY', 'MAROON', 'OLIVE', 'TEAL', 'SILVER', 'GOLD'\n",
    "}\n",
    "\n",
    "def extract_color_and_imprint(html_path):\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file.read(), 'html.parser')\n",
    "\n",
    "    td_elements = soup.find_all('td')\n",
    "\n",
    "    detected_color = None\n",
    "    detected_imprint = None\n",
    "\n",
    "    for td in td_elements:\n",
    "        text = td.get_text(separator=\" \", strip=True)\n",
    "        words = re.split(r'\\s+', text)\n",
    "\n",
    "        # Look for colors\n",
    "        for word in words:\n",
    "            clean_word = word.strip().upper()\n",
    "            if clean_word in color_names:\n",
    "                detected_color = clean_word\n",
    "                break\n",
    "        \n",
    "      \n",
    "        if detected_color :\n",
    "            break\n",
    "\n",
    "    return detected_color\n",
    "\n",
    "color= extract_color_and_imprint('/Users/aditrichaudhuri/Downloads/input_base_html4.html')\n",
    "print(f\"Detected Color: {color}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0500c9b0-baf2-4d16-8c96-4745171c6397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.31.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (2024.7.4)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.31.0-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wsproto, attrs, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-25.3.0 outcome-1.3.0.post0 selenium-4.31.0 trio-0.30.0 trio-websocket-0.12.2 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "125ded1c-d3cc-4be4-ae86-423e59dddf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/lib/python3.12/site-packages (from pytesseract) (23.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pytesseract) (10.3.0)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd1fb1fe-1e10-4765-96cb-849d0eb376f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (435179356.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    brew install teserract\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "brew install teserract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b8df3-b7b1-43ce-b315-98234f7db919",
   "metadata": {},
   "source": [
    "**IMAGE BASED APPROACH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b31596-fc2c-48f0-9767-a40b63b45f9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TesseractNotFoundError",
     "evalue": "tesseract is not installed or it's not in your PATH. See README file for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytesseract/pytesseract.py:275\u001b[0m, in \u001b[0;36mrun_tesseract\u001b[0;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(cmd_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msubprocess_args())\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[1;32m   1032\u001b[0m                         restore_signals,\n\u001b[1;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:1955\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1955\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tesseract'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTesseractNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(screenshot_path)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Use Tesseract to do OCR on the screenshot\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m text \u001b[38;5;241m=\u001b[39m pytesseract\u001b[38;5;241m.\u001b[39mimage_to_string(image)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Print the raw recognized text\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- OCR Extracted Text ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytesseract/pytesseract.py:486\u001b[0m, in \u001b[0;36mimage_to_string\u001b[0;34m(image, lang, config, nice, output_type, timeout)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    484\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[0;32m--> 486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    487\u001b[0m     Output\u001b[38;5;241m.\u001b[39mBYTES: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39m(args \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m])),\n\u001b[1;32m    488\u001b[0m     Output\u001b[38;5;241m.\u001b[39mDICT: \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs)},\n\u001b[1;32m    489\u001b[0m     Output\u001b[38;5;241m.\u001b[39mSTRING: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs),\n\u001b[1;32m    490\u001b[0m }[output_type]()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytesseract/pytesseract.py:489\u001b[0m, in \u001b[0;36mimage_to_string.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    484\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    487\u001b[0m     Output\u001b[38;5;241m.\u001b[39mBYTES: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39m(args \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m])),\n\u001b[1;32m    488\u001b[0m     Output\u001b[38;5;241m.\u001b[39mDICT: \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs)},\n\u001b[0;32m--> 489\u001b[0m     Output\u001b[38;5;241m.\u001b[39mSTRING: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs),\n\u001b[1;32m    490\u001b[0m }[output_type]()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytesseract/pytesseract.py:352\u001b[0m, in \u001b[0;36mrun_and_get_output\u001b[0;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m save(image) \u001b[38;5;28;01mas\u001b[39;00m (temp_name, input_filename):\n\u001b[1;32m    342\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_filename\u001b[39m\u001b[38;5;124m'\u001b[39m: input_filename,\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m: temp_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[1;32m    350\u001b[0m     }\n\u001b[0;32m--> 352\u001b[0m     run_tesseract(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _read_output(\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextsep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    355\u001b[0m         return_bytes,\n\u001b[1;32m    356\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytesseract/pytesseract.py:280\u001b[0m, in \u001b[0;36mrun_tesseract\u001b[0;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TesseractNotFoundError()\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timeout_manager(proc, timeout) \u001b[38;5;28;01mas\u001b[39;00m error_string:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mreturncode:\n",
      "\u001b[0;31mTesseractNotFoundError\u001b[0m: tesseract is not installed or it's not in your PATH. See README file for more information."
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import re\n",
    "\n",
    "\n",
    "html_file_path = '/Users/aditrichaudhuri/Downloads/input_base_html2.html'\n",
    "\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "\n",
    "driver.get('file://' + html_file_path)\n",
    "\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "screenshot_path = 'screenshot.png'\n",
    "driver.save_screenshot(screenshot_path)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "image = Image.open(screenshot_path)\n",
    "\n",
    "\n",
    "text = pytesseract.image_to_string(image)\n",
    "\n",
    "\n",
    "print(\"--- OCR Extracted Text ---\")\n",
    "print(text)\n",
    "\n",
    "\n",
    "color = None\n",
    "imprint_method = None\n",
    "\n",
    "# A basic list of colors\n",
    "color_names = {\n",
    "    'BLACK', 'WHITE', 'RED', 'BLUE', 'GREEN', 'YELLOW', 'ORANGE', 'PURPLE', 'PINK', 'BROWN', 'GRAY',\n",
    "    'LIME', 'CYAN', 'NAVY', 'MAROON', 'OLIVE', 'TEAL', 'SILVER', 'GOLD'\n",
    "}\n",
    "\n",
    "for line in text.splitlines():\n",
    "    words = line.strip().upper().split()\n",
    "\n",
    "    # Find color\n",
    "    for word in words:\n",
    "        if word in color_names:\n",
    "            color = word\n",
    "            break\n",
    "\n",
    "\n",
    "    if color :\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"\\n--- Extracted Information ---\")\n",
    "print(f\"Detected Color: {color}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd49331-11c2-4386-a870-e81b30c22e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION\n",
    "#input: html files\n",
    "#output: list of all colors/imprint methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
